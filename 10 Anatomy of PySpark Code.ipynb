{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6043bf83-3879-41b2-876f-1d42e511b293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Anatomy of PySpark Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4807744-c5c7-4d39-a8fc-4d0f14f0b578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In this notebook, you'll see examples of how PySpark code is structured. \n",
    "The purpose is to familiarize you with the syntax, SparkSession, method chaining, and when imports are necessary.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f227fdb3-60f2-4023-b7bb-2ea18861b33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SparkSession and Built-in DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7bf858-3ee5-45fb-adac-de5ec66b10b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The `spark` object (SparkSession) provides core operations like reading, filtering, and writing DataFrames. \n",
    "These methods are built into the SparkSession and do not require additional imports.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7653438-6216-4c5f-a846-dde2651ad2c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# In Databricks, the SparkSession is automatically available as 'spark'.\n",
    "# No need to import DataFrame methods.\n",
    "df = spark.read.format(\"csv\").load(\"/path/to/file.csv\")\n",
    "df_filtered = df.filter(\"Age > 30\")\n",
    "df_filtered.write.format(\"csv\").save(\"/path/to/directory/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cedb95e1-d352-4ff3-87df-2b8ac690cfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## When You Need to Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c51bd2d-cccd-4187-bd1d-4e7259638c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Some operations require importing modules, such as built-in functions.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Example: Using the 'col' function to reference a column\n",
    "df = df.withColumn(\"AgePlusOne\", F.col(\"Age\") + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b74a13-c5ea-4e75-a8a8-4f84e4beaea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you need to define a schema, you must also import types from 'pyspark.sql.types'.\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Example of defining a schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", StringType(), True)\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23cd0374-4820-47c0-ba19-c85724757abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Methods and Dot Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31dff204-9888-4b66-9de3-dcf0b201bc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a CSV file into a DataFrame using the SparkSession.\n",
    "# The format is specified as 'csv' and the file path is provided.\n",
    "df = spark.read.format(\"csv\").load(\"/path/to/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abbb2fa2-8246-4e35-9ac0-f51f21637d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where the 'Age' column is greater than 30.\n",
    "# The result is stored in a new DataFrame called df_filtered.\n",
    "df_filtered = df.filter(\"Age > 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3142ae4-0304-4448-99da-9e8c6991d5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the filtered DataFrame to a specified directory in CSV format.\n",
    "# The directory path is provided as the save location.\n",
    "df_filtered.write.format(\"csv\").save(\"/path/to/directory/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c359c0-0468-4300-9d4a-a38a97b9b395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In this example, we performed operations step-by-step, storing results in separate variables.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5f6914-a601-4ac8-a7d3-d9fdddadfa45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chaining Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab350aaa-dcbb-4231-8dc3-93e14fa7768d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Here, each method is chained onto the next using dot notation. The order matters, as operations are executed in sequence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a263b46-bb6a-4254-9501-4f902dbd9bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Methods can also be chained together for conciseness.\n",
    "spark.read.format(\"csv\").load(\"/path/to/file.csv\").filter(\"Age > 30\").write.format(\"csv\").save(\"/path/to/directory/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb819a61-32dd-448d-87ca-d11add7c0e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Backslash (\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1b3e66-34ec-49dd-96b3-025fd90694c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# For readability, you can use a backslash to split long chained operations across multiple lines.\n",
    "spark.\\\n",
    "    read.format(\"csv\").load(\"/path/to/file.csv\").\\\n",
    "    filter(\"Age > 30\").\\\n",
    "    write.format(\"csv\").save(\"/path/to/directory/\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anatomy of PySpark Code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
